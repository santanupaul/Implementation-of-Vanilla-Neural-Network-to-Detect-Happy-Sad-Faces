{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import sklearn\n",
    "import random \n",
    "from sklearn import datasets\n",
    "from sklearn import linear_model\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "import seaborn as sns\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_weights(num_input_features, num_hidden_units, num_output_units):\n",
    "    \"\"\"initialize weights uniformly randomly with small values\"\"\"\n",
    "    w1 = np.random.uniform(-1.0, 1.0, size=num_hidden_units*(num_input_features + 1)\n",
    "                          ).reshape(num_hidden_units, num_input_features + 1)\n",
    "    w2 = np.random.uniform(-1.0, 1.0, size=num_output_units*(num_hidden_units+1)).reshape(num_output_units, num_hidden_units+ 1)\n",
    "    return w1, w2\n",
    "# w1, w2 = init_weights(784, 30, 10)\n",
    "# print(w1.shape) # expect \n",
    "# print(w2.shape) # expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def encode_labels(y):\n",
    "        \"\"\" Encode labels into a one-hot representation\n",
    "            Params:\n",
    "            y: array of num_samples, contains the target class labels for each training example.\n",
    "            For example, y = [2, 1, 3, 3] -> 4 training samples, and the ith sample has label y[i]\n",
    "            k: number of output labels\n",
    "            returns: onehot, a matrix of labels by samples. For each column, the ith index will be\n",
    "            \"hot\", or 1, to represent that index being the label.\n",
    "        \"\"\"\n",
    "        onehot = np.zeros((len(np.unique(y)), y.shape[0]))\n",
    "#         print('onehot >>> ', onehot.shape, 'y.shape[0] >>> ', y.shape[0], 'y distinct', set(y))\n",
    "        for i in range(y.shape[0]):\n",
    "            onehot[y[i], i] = 1.0\n",
    "        return onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(v):\n",
    "        \"\"\"Calculates the softmax function that outputs a vector of values that sum to one.\n",
    "            We take max(softmax(v)) to be the predicted label. The output of the softmax function\n",
    "            is also used to calculate the cross-entropy loss\n",
    "        \"\"\"\n",
    "        logC = -np.max(v)\n",
    "        return np.exp(v + logC)/np.sum(np.exp(v + logC), axis = 0)\n",
    "    \n",
    "        \"\"\"The formula is mathematically equivalent to standard softmax function. However, since exponentials can be large\n",
    "            and unstable to compute the ratio, individual values are subtracted from the max value \n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tanh(z, deriv=False):\n",
    "        \"\"\" Compute the tanh function or its derivative.\n",
    "        \"\"\"\n",
    "        return np.tanh(z) if not deriv else 1 - np.square(np.tanh(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Relu function has been working better in recent times since it is a constant slope increasing function unlike\n",
    "    sigmoid or tanh where the slope becomes close to 0 in the extreme values\"\"\"\n",
    "def relu(z, deriv = False):\n",
    "        if not deriv:\n",
    "            relud = z\n",
    "            relud[relud < 0] = 0\n",
    "            return relud\n",
    "        deriv = z\n",
    "        deriv[deriv <= 0] = 0\n",
    "        deriv[deriv > 0] = 1\n",
    "        return deriv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigm(z, deriv = False):\n",
    "    \n",
    "    sig = 1/(1+np.exp(-z))\n",
    "    return sig if not deriv else sig*(1-sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_bias_unit(X, column=True):\n",
    "        \"\"\"Adds a bias unit to our inputs\"\"\"\n",
    "        if column:\n",
    "            bias_added = np.ones((X.shape[0], X.shape[1] + 1))\n",
    "            bias_added[:, 1:] = X\n",
    "        else:\n",
    "            bias_added = np.ones((X.shape[0] + 1, X.shape[1]))\n",
    "            bias_added[1:, :] = X\n",
    "\n",
    "        return bias_added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward(X, w1, w2, activation, do_dropout = True):\n",
    "        \"\"\" Compute feedforward step\n",
    "            Params:\n",
    "            X: matrix of num_samples by num_features, input layer with samples and features\n",
    "            w1: matrix of weights from input layer to hidden layer. Dimensionality of \n",
    "            num_hidden_units by num_features + 1 (bias)\n",
    "            w2: matrix of weights from hidden layer to output layer. Dimensionality of \n",
    "            num_output_units (equal to num class labels) by num_hidden units + 1 (bias)\n",
    "            dropout: If true, randomly set half of the activations to zero to prevent overfitting.\n",
    "        \"\"\"\n",
    "        #the activation of the input layer is simply the input matrix plus bias unit, added for each sample.\n",
    "        z1 = add_bias_unit(X)\n",
    "#         if self.dropout and do_dropout: a1 = self.compute_dropout(a1)\n",
    "        #the input of the hidden layer is obtained by applying our weights to our inputs. \n",
    "#             We essentially take a linear combination of our inputs\n",
    "        z2 = w1.dot(z1.T)\n",
    "        #applies the tanh function to obtain the input mapped to a distrubution of values between -1 and 1\n",
    "        if activation == 'tanh':\n",
    "            z3 = tanh(z2)\n",
    "        elif activation == 'sigmoid':\n",
    "            z3 = sigm(z2)\n",
    "        elif activation == 'relu':\n",
    "            z3 = relu(z2)\n",
    "        #add a bias unit to activation of the hidden layer.\n",
    "        z3 = add_bias_unit(z3, column=False)\n",
    "#         if self.dropout and do_dropout: a2 = self.compute_dropout(a2)\n",
    "        # compute input of output layer in exactly the same manner.\n",
    "        z4 = w2.dot(z3)\n",
    "        # the activation of our output layer is just the softmax function.\n",
    "        z5 = softmax(z4)\n",
    "        return z1, z2, z3, z4, z5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cost(y_enc, output, w1, w2):\n",
    "        \"\"\" Compute the cost function.\n",
    "            Params:\n",
    "            y_enc: array of num_labels x num_samples. class labels one-hot encoded\n",
    "            output: matrix of output_units x samples - activation of output layer from feedforward\n",
    "            w1: weight matrix of input to hidden layer\n",
    "            w2: weight matrix of hidden to output layer\n",
    "            \"\"\"\n",
    "        cost = - np.sum(y_enc*np.log(output))\n",
    "        # add the L2 regularization by taking the L2-norm of the weights and multiplying it with our constant.\n",
    "        l2_term = (l2/2.0) * (np.sum(np.square(w1[:, 1:])) + np.sum(np.square(w2[:, 1:])))\n",
    "        cost = cost + l2_term\n",
    "        return cost/y_enc.shape[1] #Average Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backprop(z1, z3, z5, z2, y_enc, w1, w2, activation):\n",
    "        \"\"\" Computes the gradient using backpropagation\n",
    "            Params:\n",
    "            a1: array of n_samples by features+1 - activation of input layer (just input plus bias)\n",
    "            a2: activation of hidden layer\n",
    "            a3: activation of output layer\n",
    "            z2: input of hidden layer\n",
    "            y_enc: onehot encoded class labels\n",
    "            w1: weight matrix of input layer to hidden layer\n",
    "            w2: weight matrix of hidden to output layer\n",
    "            returns: grad1, grad2: gradient of weight matrix w1, gradient of weight matrix w2\n",
    "        \"\"\"\n",
    "        #backpropagate our error\n",
    "        sigma3 = z5 - y_enc\n",
    "        z2 = add_bias_unit(z2, column=False)\n",
    "        if activation == 'tanh':\n",
    "            sigma2 = w2.T.dot(sigma3) * tanh(z2, deriv=True)\n",
    "        elif activation == 'sigmoid':\n",
    "            sigma2 = w2.T.dot(sigma3) * sigm(z2, deriv=True)\n",
    "        elif activation == 'relu':\n",
    "            sigma2 = w2.T.dot(sigma3) * relu(z2, deriv=True)\n",
    "        #get rid of the bias row\n",
    "        sigma2 = sigma2[1:, :]\n",
    "        grad1 = sigma2.dot(z1)\n",
    "        grad2 = sigma3.dot(z3.T)\n",
    "         # add the regularization term\n",
    "        grad1[:, 1:]+= (w1[:, 1:]*l2) # derivative of .5*l2*w1^2\n",
    "        grad2[:, 1:]+= (w2[:, 1:]*l2) # derivative of .5*l2*w2^2\n",
    "        return grad1, grad2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_nn(X, w1, w2, activation, dropout = False):\n",
    "        \"\"\"Generate a set of predicted labels for the input dataset\"\"\"\n",
    "        z1, z2, z3, z4, z5 = forward(X, w1, w2, activation, do_dropout = dropout)\n",
    "        #z5 is of dimension output units x num_samples. each row is an array representing the likelihood that the sample belongs to the class label given by the index...\n",
    "        #ex: first row of z5 = [0.98, 0.78, 0.36]. This means our network has 3 output units = 3 class labels. And this instance most likely belongs to the class given by the label 0.\n",
    "        y_pred = np.argmax(z5, axis = 0)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(X_train, y_train, w1, w2, activation):\n",
    "        \"\"\"Calculate the training accuracy. Requires passing through the entire dataset.\"\"\"\n",
    "#         print('w1 >>> ', w1, 'w2 >>> ', w2)\n",
    "        y_train_pred = predict_nn(X = X_train, w1 = w1, w2 = w2, activation = activation)\n",
    "        diffs = y_train_pred - y_train\n",
    "        count = 0.\n",
    "        for i in range(y_train.shape[0]):\n",
    "            if diffs[i] != 0:\n",
    "                count+=1\n",
    "        return 100 - count*100/y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(X_data, y_data, X_test, y_test, n_output, n_features, n_hidden, l2, epochs, learning_rate, momentum_const, \n",
    "                decay_rate, minibatch_size, optimizer, activation, check_gradients, nesterov, print_progress):\n",
    "   \n",
    "    y_enc = encode_labels(y_data)\n",
    "    \n",
    "    #     Intialize Weights\n",
    "#     w1, w2 = init_weights(n_features, n_hidden, n_output)\n",
    "    w1, w2 = wt1, wt2\n",
    "\n",
    "    \"\"\" Learn weights from training data\n",
    "        Params:\n",
    "        X: matrix of samples x features. Input layer\n",
    "        y: target class labels of the training instances (ex: y = [1, 3, 4, 4, 3])\n",
    "        print_progress: True if you want to see the loss and training accuracy, but it is expensive.\n",
    "    \"\"\"\n",
    "    \n",
    "    accs = []\n",
    "    \n",
    "    # PREVIOUS GRADIENTS\n",
    "    prev_grad_w1 = np.zeros(w1.shape)\n",
    "    prev_grad_w2 = np.zeros(w2.shape)\n",
    "    # PREVIOUS WEIGHTS(MOMENTUM)\n",
    "    prev_w1 = np.zeros(w1.shape)\n",
    "    prev_w2 = np.zeros(w2.shape)\n",
    "    prev2_w1 = np.zeros(w1.shape)\n",
    "    prev2_w2 = np.zeros(w2.shape)\n",
    "    print(\"fitting\")\n",
    "    costs = []\n",
    "    grad_1_li, grad_2_li = [], [] # used to keep list of gradients which can be used to measure and differentiate between learning speed of input -> hidden and hidden -> output layer weights\n",
    "\n",
    "    #     supported_optimizers = ['Gradient Descent', 'Momentum', 'Nesterov', 'Adam', 'Adagrad', 'Adadelta', 'RMSProp']\n",
    "    supported_optimizers = ['Gradient Descent', 'Momentum']\n",
    "    if optimizer not in supported_optimizers:\n",
    "        print(\"Error: unsupported optimizer requested.\")\n",
    "        print(\"Available optimizers: {}\".format(supported_optimizers))\n",
    "    #         exit()\n",
    "\n",
    "    #     supported_activations = ['relu', 'tanh', 'sigmoid', 'maxout', 'elu']\n",
    "    supported_activations = ['relu', 'tanh', 'sigmoid']\n",
    "    if activation not in supported_activations:\n",
    "        print(\"Error: unsupported activation requested.\")\n",
    "        print(\"Available activations: {}\".format(supported_activations))\n",
    "\n",
    "    #pass through the dataset\n",
    "    for i in range(epochs):\n",
    "        previous_accuracies = []\n",
    "        learning_rate /= (1 + decay_rate*i)\n",
    "        mini = np.array_split(range(y_data.shape[0]), minibatch_size)\n",
    "        grads_w1, grads_w2 = [], [] # needed if we want to remember averages of gradients across time\n",
    "\n",
    "        for idx in mini:\n",
    "            #feed feedforward\n",
    "            z1, z2, z3, z4, z5 = forward(X_data[idx], w1, w2, activation)\n",
    "\n",
    "            cost = get_cost(y_enc = y_enc[:, idx], output = z5, w1 = w1, w2 = w2)\n",
    "            costs.append(cost)\n",
    "\n",
    "            #compute gradient via backpropagation\n",
    "\n",
    "            grad1, grad2 = backprop(z1=z1, z3=z3, z5=z5, z2=z2, y_enc=y_enc[:, idx], w1=w1, w2=w2, activation=activation)\n",
    "            grad_1_li.append(grad1)\n",
    "            grad_2_li.append(grad2)\n",
    "\n",
    "            # update parameters, multiplying by learning rate + momentum constants\n",
    "            # w1_update, w2_update = self.momentum_optimizer(self.learning_rate, grad1, grad2)\n",
    "            w1_update, w2_update = learning_rate*grad1, learning_rate*grad2\n",
    "\n",
    "            prev2_w1, prev2_w2 = prev_w1, prev_w2\n",
    "            prev_w1, prev_w2 = w1, w2\n",
    "            if nesterov:\n",
    "                print('NOT AVAILABLE NOW')\n",
    "                # v_prev = v # back this up\n",
    "                # v = mu * v - learning_rate * dx # velocity update stays the same\n",
    "                # x += -mu * v_prev + (1 + mu) * v # position update changes form\n",
    "                # psuedocode from http://cs231n.github.io/neural-networks-3/#sgd\n",
    "    #                 v1 = momentum_const * prev_grad_w1 - w1_update\n",
    "    #                 v2 = momentum_const * prev_grad_w2 - w2_update\n",
    "    #                 w1 += -self.momentum_const * prev_grad_w1 + (1 + self.momentum_const) * v1\n",
    "    #                 w2 += -self.momentum_const * prev_grad_w2 + (1 + self.momentum_const) * v2\n",
    "            else:\n",
    "                # gradient update: w += -alpha * gradient.\n",
    "                # use momentum - add in previous gradient mutliplied by a momentum hyperparameter.\n",
    "                w1 += -((1-momentum_const)*w1_update + (momentum_const*(prev2_w1 - prev_w1)))\n",
    "                w2 += -((1-momentum_const)*w2_update + (momentum_const*(prev2_w2 - prev_w2)))\n",
    "\n",
    "            acc_train = accuracy(X_data, y_data, w1, w2, activation)\n",
    "            acc_test = accuracy(X_test, y_test, w1, w2, activation)\n",
    "            \n",
    "            print('Epoch: ', i)\n",
    "            print('Training Acc: ', acc_train, '\\nTest Acc: ', acc_test)\n",
    "            accs.append([acc_train, acc_test])\n",
    "#         if print_progress and (i+1) % 50==0:\n",
    "#             print(\"Epoch: {}\".format(i + 1))\n",
    "#             print(\"Loss: {}\".format(cost))\n",
    "    #             if self.check_gradients:\n",
    "    #                 print(\"Gradient Error: {}\".format(w1_grad_error))\n",
    "#             grad_1_mag, grad_2_mag = np.linalg.norm(grad_1_li), np.linalg.norm(grad_2_li)\n",
    "#             print(\"grad1 mag: {}, grad2 mang: {}\".format(grad_1_mag, grad_2_mag))\n",
    "    #         print('w1 >>> ', w1, 'w2 >>> ', w2)\n",
    "#             acc = accuracy(X_data, Y_data, w1, w2)\n",
    "#             previous_accuracies.append(acc)\n",
    "    #             if early_stop is not None and len(previous_accuracies) > 3:\n",
    "    #                 if abs(previous_accuracies[-1] - previous_accuracies[-2]) < self.early_stop and abs(previous_accuracies[-1] - previous_accuracies[-3]) < self.early_stop:\n",
    "    #                     print(\"Early stopping, accuracy has stayed roughly constant over last 100 iterations.\")\n",
    "    #                     break\n",
    "\n",
    "\n",
    "#             print(\"Training Accuracy: {}\".format(acc))\n",
    "\n",
    "    # # Assign new parameters to the model\n",
    "#     model = { 'W1': w1[:, :2], 'b1': w1[:, 2], 'W2': w2[:, :2], 'b2': w2[:, 2]}\n",
    "    model = { 'W1': w1, 'W2': w2, 'Acc': accs}\n",
    "        \n",
    "    return [model]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # t0 = time.time()\n",
    "\n",
    "# def kfold_nn(X_copy, y_copy, n_output, n_features, n_hidden, l2, epochs, learning_rate, momentum_const, \n",
    "#                       decay_rate, minibatch_size, optimizer, activation, check_gradients, nesterov, print_progress, \n",
    "#                       k = 5):\n",
    "\n",
    "#     train_cv_acc = []\n",
    "#     test_cv_acc = []\n",
    "#     # cv_acc = []\n",
    "    \n",
    "#     X_folds = np.array_split(X_copy, k)\n",
    "#     y_folds = np.array_split(y_copy, k)\n",
    "\n",
    "#     for cnt in range(len(X_folds)):\n",
    "#     # for cnt in range(1):\n",
    "#     #     cnt = 2\n",
    "#         X_train = list(X_folds)\n",
    "#         X_test = X_train.pop(cnt)\n",
    "#         X_train = np.concatenate(X_train)\n",
    "\n",
    "#         y_train = list(y_folds)\n",
    "#         y_test = y_train.pop(cnt)\n",
    "#         y_train = np.concatenate(y_train)\n",
    "        \n",
    "#         model_train = build_model(X_train, y_train, n_output, n_features, n_hidden, l2, epochs, learning_rate, momentum_const, decay_rate, minibatch_size, \n",
    "#                    optimizer, activation, check_gradients, nesterov, print_progress)\n",
    "\n",
    "#         acc_train = accuracy(X_train, y_train, model_train[0]['W1'], model_train[0]['W2'], activation)\n",
    "#         acc_test = accuracy(X_test, y_test, model_train[0]['W1'], model_train[0]['W2'], activation)\n",
    "        \n",
    "#         print('k = ', cnt+1)\n",
    "#         print('Training Accuracy: ', acc_train)\n",
    "#         print('\\tTest Accuracy: ', acc_test)\n",
    "\n",
    "#         train_cv_acc.append(acc_train)\n",
    "#         test_cv_acc.append(acc_test)\n",
    "\n",
    "\n",
    "#     return np.mean(train_cv_acc), np.std(train_cv_acc), np.mean(test_cv_acc), np.std(test_cv_acc)\n",
    "\n",
    "    \n",
    "# # t1 = time.time()\n",
    "\n",
    "# # print('Time Taken >> ', t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kfold_nn(X_copy, y_copy, X_test, y_test, n_output, n_features, n_hidden, l2, epochs, learning_rate, momentum_const, \n",
    "                      decay_rate, minibatch_size, optimizer, activation, check_gradients, nesterov, print_progress, \n",
    "                      k = 5):\n",
    "\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X_copy, y_copy, test_size = 0.25)\n",
    "\n",
    "#     for cnt in range(len(X_folds)):\n",
    "    for cnt in range(1):\n",
    "        cnt = 2\n",
    "#         X_train = list(X_folds)\n",
    "#         X_test = X_train.pop(cnt)\n",
    "#         X_train = np.concatenate(X_train)\n",
    "\n",
    "#         y_train = list(y_folds)\n",
    "#         y_test = y_train.pop(cnt)\n",
    "#         y_train = np.concatenate(y_train)\n",
    "        \n",
    "#         model_train = build_model(X_train, y_train, X_test, y_test, n_output, n_features, n_hidden, l2, epochs, learning_rate, momentum_const, decay_rate, minibatch_size, \n",
    "#                    optimizer, activation, check_gradients, nesterov, print_progress)\n",
    "        model_train = build_model(X_copy, y_copy, X_test, y_test, n_output, n_features, n_hidden, l2, epochs, learning_rate, momentum_const, decay_rate, minibatch_size, \n",
    "                   optimizer, activation, check_gradients, nesterov, print_progress)\n",
    "\n",
    "#         acc_train = accuracy(X_train, y_train, model_train[0]['W1'], model_train[0]['W2'], activation)\n",
    "#         acc_test = accuracy(X_test, y_test, model_train[0]['W1'], model_train[0]['W2'], activation)\n",
    "        \n",
    "#         print('k = ', cnt+1)\n",
    "#         print('Training Accuracy: ', acc_train)\n",
    "#         print('\\tTest Accuracy: ', acc_test)\n",
    "\n",
    "#         train_cv_acc.append(acc_train)\n",
    "#         test_cv_acc.append(acc_test)\n",
    "\n",
    "    return model_train[0]['W1'], model_train[0]['W2'], model_train[0]['Acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/santanupaul/Documents/Personal/Masters in Analytics/UConn/Study Related/Python/Project/fer2013\r\n"
     ]
    }
   ],
   "source": [
    "# Change the Current Path\n",
    "os.chdir('/Users/santanupaul/Documents/Personal/Masters in Analytics/UConn/Study Related/Python/Project/fer2013')\n",
    "\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion                                             pixels     Usage\n",
       "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
       "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
       "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
       "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
       "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the pixel matrix \n",
    "df = pd.read_csv('fer2013.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35887, 3)\n",
      "(15066, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/santanupaul/anaconda/lib/python3.6/site-packages/pandas/core/generic.py:2999: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df1 = df[(df['emotion'] == 3) | (df['emotion'] == 4)] # 3 happy, and 4 Sad\n",
    "df1.emotion = df1.emotion - 3\n",
    "print(df1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the pixel columns to form different fields for each pixel\n",
    "df1 = pd.concat([df1[['emotion']], df1['pixels'].str.split(\" \", expand = True)], axis = 1)\n",
    "df1.head()\n",
    "\n",
    "X = df1.iloc[:, 1:].values #Store as numpy array\n",
    "Y = df1.iloc[:, 0].values\n",
    "\n",
    "X = X.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/santanupaul/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py:429: DataConversionWarning:\n",
      "\n",
      "Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Standardization: Although data is in the same scale (values from 1 to 255) mean = 0 and std = 1 is recommended for most\n",
    "# machine learning algorithms\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_std = StandardScaler().fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/santanupaul/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning:\n",
      "\n",
      "This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "\n",
      "/Users/santanupaul/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py:429: DataConversionWarning:\n",
      "\n",
      "Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9641, 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the 3 dimensionality reduction methods\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "\n",
    "# X_std.shape\n",
    "X_temp, X_tst, Y_temp, Y_tst = train_test_split(X_std, Y, test_size = 0.2, random_state = 51)\n",
    "X_trn, X_val, Y_trn, Y_val = train_test_split(X_temp, Y_temp, test_size = 0.2, random_state = 51)\n",
    "lda = LDA(n_components=1)\n",
    "# # Taking in as second argument the Target as labels\n",
    "X_LDA_2D = lda.fit_transform(X_trn, Y_trn)\n",
    "X_LDA_val = lda.transform(X_val)\n",
    "X_LDA_tst = lda.transform(X_tst)\n",
    "X_LDA_2D.shape # Getting only one linear discriminant since number of classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Nodes in Hidden Layer =  50\n",
      "fitting\n",
      "Epoch:  0\n",
      "Training Acc:  81.13266258686858 \n",
      "Test Acc:  65.74035669846538\n",
      "Epoch:  1\n",
      "Training Acc:  81.13266258686858 \n",
      "Test Acc:  65.74035669846538\n",
      "Epoch:  2\n",
      "Training Acc:  81.13266258686858 \n",
      "Test Acc:  65.74035669846538\n",
      "Epoch:  3\n",
      "Training Acc:  81.12229021885696 \n",
      "Test Acc:  65.74035669846538\n",
      "Epoch:  4\n",
      "Training Acc:  81.12229021885696 \n",
      "Test Acc:  65.74035669846538\n",
      "Epoch:  5\n",
      "Training Acc:  81.12229021885696 \n",
      "Test Acc:  65.74035669846538\n",
      "Epoch:  6\n",
      "Training Acc:  81.12229021885696 \n",
      "Test Acc:  65.74035669846538\n",
      "Epoch:  7\n",
      "Training Acc:  81.12229021885696 \n",
      "Test Acc:  65.74035669846538\n",
      "Epoch:  8\n",
      "Training Acc:  81.12229021885696 \n",
      "Test Acc:  65.74035669846538\n",
      "Epoch:  9\n",
      "Training Acc:  81.12229021885696 \n",
      "Test Acc:  65.74035669846538\n",
      "Epoch:  10\n",
      "Training Acc:  81.12229021885696 \n",
      "Test Acc:  65.74035669846538\n",
      "Epoch:  11\n",
      "Training Acc:  81.12229021885696 \n",
      "Test Acc:  65.74035669846538\n",
      "Epoch:  12\n",
      "Training Acc:  81.12229021885696 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  13\n",
      "Training Acc:  81.12229021885696 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  14\n",
      "Training Acc:  81.12229021885696 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  15\n",
      "Training Acc:  81.12229021885696 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  16\n",
      "Training Acc:  81.12229021885696 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  17\n",
      "Training Acc:  81.12229021885696 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  18\n",
      "Training Acc:  81.12229021885696 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  19\n",
      "Training Acc:  81.12229021885696 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  20\n",
      "Training Acc:  81.13266258686858 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  21\n",
      "Training Acc:  81.13266258686858 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  22\n",
      "Training Acc:  81.13266258686858 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  23\n",
      "Training Acc:  81.13266258686858 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  24\n",
      "Training Acc:  81.13266258686858 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  25\n",
      "Training Acc:  81.13266258686858 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  26\n",
      "Training Acc:  81.13266258686858 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  27\n",
      "Training Acc:  81.13266258686858 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  28\n",
      "Training Acc:  81.13266258686858 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  29\n",
      "Training Acc:  81.12229021885696 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  30\n",
      "Training Acc:  81.11191785084534 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  31\n",
      "Training Acc:  81.11191785084534 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  32\n",
      "Training Acc:  81.11191785084534 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  33\n",
      "Training Acc:  81.11191785084534 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  34\n",
      "Training Acc:  81.11191785084534 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  35\n",
      "Training Acc:  81.11191785084534 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  36\n",
      "Training Acc:  81.11191785084534 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  37\n",
      "Training Acc:  81.10154548283373 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  38\n",
      "Training Acc:  81.10154548283373 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  39\n",
      "Training Acc:  81.10154548283373 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  40\n",
      "Training Acc:  81.10154548283373 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  41\n",
      "Training Acc:  81.10154548283373 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  42\n",
      "Training Acc:  81.10154548283373 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  43\n",
      "Training Acc:  81.09117311482211 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  44\n",
      "Training Acc:  81.09117311482211 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  45\n",
      "Training Acc:  81.09117311482211 \n",
      "Test Acc:  65.74035669846538\n",
      "Epoch:  46\n",
      "Training Acc:  81.09117311482211 \n",
      "Test Acc:  65.74035669846538\n",
      "Epoch:  47\n",
      "Training Acc:  81.09117311482211 \n",
      "Test Acc:  65.74035669846538\n",
      "Epoch:  48\n",
      "Training Acc:  81.09117311482211 \n",
      "Test Acc:  65.74035669846538\n",
      "Epoch:  49\n",
      "Training Acc:  81.09117311482211 \n",
      "Test Acc:  65.74035669846538\n",
      "Epoch:  50\n",
      "Training Acc:  81.09117311482211 \n",
      "Test Acc:  65.74035669846538\n",
      "Epoch:  51\n",
      "Training Acc:  81.09117311482211 \n",
      "Test Acc:  65.74035669846538\n",
      "Epoch:  52\n",
      "Training Acc:  81.09117311482211 \n",
      "Test Acc:  65.74035669846538\n",
      "Epoch:  53\n",
      "Training Acc:  81.09117311482211 \n",
      "Test Acc:  65.74035669846538\n",
      "Epoch:  54\n",
      "Training Acc:  81.09117311482211 \n",
      "Test Acc:  65.74035669846538\n",
      "Epoch:  55\n",
      "Training Acc:  81.09117311482211 \n",
      "Test Acc:  65.74035669846538\n",
      "Epoch:  56\n",
      "Training Acc:  81.09117311482211 \n",
      "Test Acc:  65.74035669846538\n",
      "Epoch:  57\n",
      "Training Acc:  81.09117311482211 \n",
      "Test Acc:  65.74035669846538\n",
      "Epoch:  58\n",
      "Training Acc:  81.09117311482211 \n",
      "Test Acc:  65.74035669846538\n",
      "Epoch:  59\n",
      "Training Acc:  81.09117311482211 \n",
      "Test Acc:  65.74035669846538\n",
      "Epoch:  60\n",
      "Training Acc:  81.09117311482211 \n",
      "Test Acc:  65.74035669846538\n",
      "Epoch:  61\n",
      "Training Acc:  81.09117311482211 \n",
      "Test Acc:  65.74035669846538\n",
      "Epoch:  62\n",
      "Training Acc:  81.09117311482211 \n",
      "Test Acc:  65.74035669846538\n",
      "Epoch:  63\n",
      "Training Acc:  81.09117311482211 \n",
      "Test Acc:  65.74035669846538\n",
      "Epoch:  64\n",
      "Training Acc:  81.09117311482211 \n",
      "Test Acc:  65.74035669846538\n",
      "Epoch:  65\n",
      "Training Acc:  81.09117311482211 \n",
      "Test Acc:  65.74035669846538\n",
      "Epoch:  66\n",
      "Training Acc:  81.10154548283373 \n",
      "Test Acc:  65.74035669846538\n",
      "Epoch:  67\n",
      "Training Acc:  81.10154548283373 \n",
      "Test Acc:  65.74035669846538\n",
      "Epoch:  68\n",
      "Training Acc:  81.10154548283373 \n",
      "Test Acc:  65.74035669846538\n",
      "Epoch:  69\n",
      "Training Acc:  81.09117311482211 \n",
      "Test Acc:  65.74035669846538\n",
      "Epoch:  70\n",
      "Training Acc:  81.10154548283373 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  71\n",
      "Training Acc:  81.10154548283373 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  72\n",
      "Training Acc:  81.10154548283373 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  73\n",
      "Training Acc:  81.09117311482211 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  74\n",
      "Training Acc:  81.09117311482211 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  75\n",
      "Training Acc:  81.09117311482211 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  76\n",
      "Training Acc:  81.09117311482211 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  77\n",
      "Training Acc:  81.09117311482211 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  78\n",
      "Training Acc:  81.09117311482211 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  79\n",
      "Training Acc:  81.09117311482211 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  80\n",
      "Training Acc:  81.09117311482211 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  81\n",
      "Training Acc:  81.09117311482211 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  82\n",
      "Training Acc:  81.0808007468105 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  83\n",
      "Training Acc:  81.0808007468105 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  84\n",
      "Training Acc:  81.0808007468105 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  85\n",
      "Training Acc:  81.0808007468105 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  86\n",
      "Training Acc:  81.0808007468105 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  87\n",
      "Training Acc:  81.0808007468105 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  88\n",
      "Training Acc:  81.0808007468105 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  89\n",
      "Training Acc:  81.0808007468105 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  90\n",
      "Training Acc:  81.0808007468105 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  91\n",
      "Training Acc:  81.0808007468105 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  92\n",
      "Training Acc:  81.0808007468105 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  93\n",
      "Training Acc:  81.0808007468105 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  94\n",
      "Training Acc:  81.0808007468105 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  95\n",
      "Training Acc:  81.0808007468105 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  96\n",
      "Training Acc:  81.0808007468105 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  97\n",
      "Training Acc:  81.0808007468105 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  98\n",
      "Training Acc:  81.0808007468105 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  99\n",
      "Training Acc:  81.0808007468105 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  100\n",
      "Training Acc:  81.0808007468105 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  101\n",
      "Training Acc:  81.0808007468105 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  102\n",
      "Training Acc:  81.0808007468105 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  103\n",
      "Training Acc:  81.0808007468105 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  104\n",
      "Training Acc:  81.0808007468105 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  105\n",
      "Training Acc:  81.0808007468105 \n",
      "Test Acc:  65.78183326420572\n",
      "Epoch:  106\n",
      "Training Acc:  81.0808007468105 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  107\n",
      "Training Acc:  81.0808007468105 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  108\n",
      "Training Acc:  81.07042837879888 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  109\n",
      "Training Acc:  81.07042837879888 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  110\n",
      "Training Acc:  81.07042837879888 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  111\n",
      "Training Acc:  81.07042837879888 \n",
      "Test Acc:  65.82330982994608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  112\n",
      "Training Acc:  81.06005601078726 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  113\n",
      "Training Acc:  81.06005601078726 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  114\n",
      "Training Acc:  81.06005601078726 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  115\n",
      "Training Acc:  81.06005601078726 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  116\n",
      "Training Acc:  81.06005601078726 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  117\n",
      "Training Acc:  81.06005601078726 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  118\n",
      "Training Acc:  81.06005601078726 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  119\n",
      "Training Acc:  81.06005601078726 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  120\n",
      "Training Acc:  81.06005601078726 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  121\n",
      "Training Acc:  81.06005601078726 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  122\n",
      "Training Acc:  81.06005601078726 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  123\n",
      "Training Acc:  81.07042837879888 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  124\n",
      "Training Acc:  81.07042837879888 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  125\n",
      "Training Acc:  81.07042837879888 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  126\n",
      "Training Acc:  81.07042837879888 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  127\n",
      "Training Acc:  81.07042837879888 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  128\n",
      "Training Acc:  81.07042837879888 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  129\n",
      "Training Acc:  81.07042837879888 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  130\n",
      "Training Acc:  81.07042837879888 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  131\n",
      "Training Acc:  81.0808007468105 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  132\n",
      "Training Acc:  81.0808007468105 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  133\n",
      "Training Acc:  81.0808007468105 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  134\n",
      "Training Acc:  81.0808007468105 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  135\n",
      "Training Acc:  81.0808007468105 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  136\n",
      "Training Acc:  81.07042837879888 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  137\n",
      "Training Acc:  81.06005601078726 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  138\n",
      "Training Acc:  81.04968364277565 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  139\n",
      "Training Acc:  81.03931127476403 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  140\n",
      "Training Acc:  81.03931127476403 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  141\n",
      "Training Acc:  81.03931127476403 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  142\n",
      "Training Acc:  81.03931127476403 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  143\n",
      "Training Acc:  81.03931127476403 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  144\n",
      "Training Acc:  81.03931127476403 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  145\n",
      "Training Acc:  81.03931127476403 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  146\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  147\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  148\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  149\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  150\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  151\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  152\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  153\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  154\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  155\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  156\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  157\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  158\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  159\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  160\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  161\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  162\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  163\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  164\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  165\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  166\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  167\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  168\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  169\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  170\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  171\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  172\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  173\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  174\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  175\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  176\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  177\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  178\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  179\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  180\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  181\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  182\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  183\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  184\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  185\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  186\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  187\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  188\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  189\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  190\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  191\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  192\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  193\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  194\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  195\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  196\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  197\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  198\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  199\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  200\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  201\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  202\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  203\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  204\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  205\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  206\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  207\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  208\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  209\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  210\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  211\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  212\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  213\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  214\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  215\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  216\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  217\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  218\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  219\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  220\n",
      "Training Acc:  81.03931127476403 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  221\n",
      "Training Acc:  81.03931127476403 \n",
      "Test Acc:  65.86478639568644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  222\n",
      "Training Acc:  81.03931127476403 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  223\n",
      "Training Acc:  81.03931127476403 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  224\n",
      "Training Acc:  81.03931127476403 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  225\n",
      "Training Acc:  81.03931127476403 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  226\n",
      "Training Acc:  81.03931127476403 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  227\n",
      "Training Acc:  81.03931127476403 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  228\n",
      "Training Acc:  81.03931127476403 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  229\n",
      "Training Acc:  81.03931127476403 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  230\n",
      "Training Acc:  81.03931127476403 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  231\n",
      "Training Acc:  81.03931127476403 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  232\n",
      "Training Acc:  81.03931127476403 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  233\n",
      "Training Acc:  81.03931127476403 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  234\n",
      "Training Acc:  81.03931127476403 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  235\n",
      "Training Acc:  81.03931127476403 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  236\n",
      "Training Acc:  81.03931127476403 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  237\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  238\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  239\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  240\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  241\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  242\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  243\n",
      "Training Acc:  81.03931127476403 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  244\n",
      "Training Acc:  81.03931127476403 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  245\n",
      "Training Acc:  81.03931127476403 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  246\n",
      "Training Acc:  81.03931127476403 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  247\n",
      "Training Acc:  81.03931127476403 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  248\n",
      "Training Acc:  81.03931127476403 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  249\n",
      "Training Acc:  81.03931127476403 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  250\n",
      "Training Acc:  81.03931127476403 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  251\n",
      "Training Acc:  81.03931127476403 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  252\n",
      "Training Acc:  81.03931127476403 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  253\n",
      "Training Acc:  81.03931127476403 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  254\n",
      "Training Acc:  81.03931127476403 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  255\n",
      "Training Acc:  81.03931127476403 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  256\n",
      "Training Acc:  81.04968364277565 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  257\n",
      "Training Acc:  81.04968364277565 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  258\n",
      "Training Acc:  81.04968364277565 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  259\n",
      "Training Acc:  81.04968364277565 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  260\n",
      "Training Acc:  81.04968364277565 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  261\n",
      "Training Acc:  81.04968364277565 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  262\n",
      "Training Acc:  81.04968364277565 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  263\n",
      "Training Acc:  81.04968364277565 \n",
      "Test Acc:  65.82330982994608\n",
      "Epoch:  264\n",
      "Training Acc:  81.04968364277565 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  265\n",
      "Training Acc:  81.04968364277565 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  266\n",
      "Training Acc:  81.04968364277565 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  267\n",
      "Training Acc:  81.04968364277565 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  268\n",
      "Training Acc:  81.04968364277565 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  269\n",
      "Training Acc:  81.03931127476403 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  270\n",
      "Training Acc:  81.03931127476403 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  271\n",
      "Training Acc:  81.03931127476403 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  272\n",
      "Training Acc:  81.03931127476403 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  273\n",
      "Training Acc:  81.03931127476403 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  274\n",
      "Training Acc:  81.03931127476403 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  275\n",
      "Training Acc:  81.03931127476403 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  276\n",
      "Training Acc:  81.03931127476403 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  277\n",
      "Training Acc:  81.03931127476403 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  278\n",
      "Training Acc:  81.03931127476403 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  279\n",
      "Training Acc:  81.03931127476403 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  280\n",
      "Training Acc:  81.03931127476403 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  281\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  282\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  283\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  284\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  285\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  286\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  287\n",
      "Training Acc:  81.03931127476403 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  288\n",
      "Training Acc:  81.03931127476403 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  289\n",
      "Training Acc:  81.03931127476403 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  290\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  291\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  292\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  293\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  294\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  295\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  296\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  297\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  298\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  299\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  300\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  301\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  302\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  303\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  304\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  305\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  306\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  307\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  308\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  309\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  310\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  311\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  312\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  313\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  314\n",
      "Training Acc:  81.00819417072918 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  315\n",
      "Training Acc:  81.00819417072918 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  316\n",
      "Training Acc:  81.00819417072918 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  317\n",
      "Training Acc:  81.00819417072918 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  318\n",
      "Training Acc:  81.00819417072918 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  319\n",
      "Training Acc:  81.00819417072918 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  320\n",
      "Training Acc:  81.00819417072918 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  321\n",
      "Training Acc:  81.00819417072918 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  322\n",
      "Training Acc:  81.00819417072918 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  323\n",
      "Training Acc:  81.00819417072918 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  324\n",
      "Training Acc:  81.00819417072918 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  325\n",
      "Training Acc:  81.00819417072918 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  326\n",
      "Training Acc:  81.00819417072918 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  327\n",
      "Training Acc:  81.00819417072918 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  328\n",
      "Training Acc:  81.00819417072918 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  329\n",
      "Training Acc:  81.00819417072918 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  330\n",
      "Training Acc:  81.00819417072918 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  331\n",
      "Training Acc:  81.00819417072918 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  332\n",
      "Training Acc:  81.00819417072918 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  333\n",
      "Training Acc:  81.00819417072918 \n",
      "Test Acc:  65.86478639568644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  334\n",
      "Training Acc:  81.00819417072918 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  335\n",
      "Training Acc:  81.00819417072918 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  336\n",
      "Training Acc:  81.00819417072918 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  337\n",
      "Training Acc:  81.00819417072918 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  338\n",
      "Training Acc:  81.00819417072918 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  339\n",
      "Training Acc:  81.00819417072918 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  340\n",
      "Training Acc:  81.00819417072918 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  341\n",
      "Training Acc:  81.00819417072918 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  342\n",
      "Training Acc:  81.00819417072918 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  343\n",
      "Training Acc:  81.00819417072918 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  344\n",
      "Training Acc:  81.00819417072918 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  345\n",
      "Training Acc:  81.00819417072918 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  346\n",
      "Training Acc:  81.00819417072918 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  347\n",
      "Training Acc:  81.00819417072918 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  348\n",
      "Training Acc:  81.00819417072918 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  349\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  350\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  351\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  352\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  353\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  354\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  355\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  356\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  357\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  358\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  359\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  360\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  361\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  362\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  363\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  364\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  365\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  366\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  367\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  368\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  369\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  370\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  371\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  372\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  373\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  374\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  375\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  376\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  377\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  378\n",
      "Training Acc:  81.04968364277565 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  379\n",
      "Training Acc:  81.04968364277565 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  380\n",
      "Training Acc:  81.04968364277565 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  381\n",
      "Training Acc:  81.04968364277565 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  382\n",
      "Training Acc:  81.04968364277565 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  383\n",
      "Training Acc:  81.04968364277565 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  384\n",
      "Training Acc:  81.04968364277565 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  385\n",
      "Training Acc:  81.04968364277565 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  386\n",
      "Training Acc:  81.04968364277565 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  387\n",
      "Training Acc:  81.04968364277565 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  388\n",
      "Training Acc:  81.03931127476403 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  389\n",
      "Training Acc:  81.03931127476403 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  390\n",
      "Training Acc:  81.03931127476403 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  391\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  392\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  393\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  394\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  395\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.86478639568644\n",
      "Epoch:  396\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  397\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  398\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  399\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  400\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  401\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  402\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  403\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  404\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  405\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  406\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  407\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  408\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.90626296142679\n",
      "Epoch:  409\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  410\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  411\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  412\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  413\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  414\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  415\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  416\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  417\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  418\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  419\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  420\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  421\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  422\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  423\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  424\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  425\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  426\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  427\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  428\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  429\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  430\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  431\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  432\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  433\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  434\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  435\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  436\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  437\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  438\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  439\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  440\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  441\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  442\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  443\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  444\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  445\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  446\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  447\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  448\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  449\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  450\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  451\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  452\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  453\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  454\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  455\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  456\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  457\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  458\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  459\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  460\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  461\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  462\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  463\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  464\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  465\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  466\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  467\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  468\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  469\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  470\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  471\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  472\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  473\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  474\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  475\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  476\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  477\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  478\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  479\n",
      "Training Acc:  81.0185665387408 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  480\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  481\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  482\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  483\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  484\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  485\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  486\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  487\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  488\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  489\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  490\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  491\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  492\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  493\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  494\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  495\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  496\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  497\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  498\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Epoch:  499\n",
      "Training Acc:  81.02893890675242 \n",
      "Test Acc:  65.94773952716716\n",
      "Time Taken >>  32.836660861968994\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "\"\"\" Feedforward neural network with a single hidden layer\n",
    "    Params:\n",
    "    n_output: int: number of output units, equal to num class labels\n",
    "    n_features: int: number of features in the input dataset\n",
    "    n_hidden: int: (default 30): num hidden units\n",
    "    l2: float(default: 0.0) - lambda value for L2 regularization\n",
    "    epochs: int (default = 500) - passes over training set\n",
    "    learning_rate: float (default: 0.001) - learning reate\n",
    "    momentum_const: float (default: 0.0) - momentum constant - multiplied with gradient of previous pass through set\n",
    "    decay_rate: float (default 0.0) - shrinks learning rate after each epoch\n",
    "    minibatch_size: int (default: 1) - divides training data into batches for efficiency\n",
    "\"\"\"\n",
    "n_output = len(set(Y_trn))\n",
    "n_features = X_LDA_2D.shape[1]\n",
    "n_hidden_min = 50\n",
    "n_hidden_num = 1\n",
    "l2 = 1000\n",
    "epochs = 500\n",
    "learning_rate = 0.000001\n",
    "momentum_const = 0\n",
    "decay_rate = 0\n",
    "minibatch_size = 1\n",
    "optimizer = 'Gradient Descent'\n",
    "activation = 'tanh'\n",
    "check_gradients = True\n",
    "nesterov = False\n",
    "print_progress = True\n",
    "\n",
    "hidden_cv = [[0 for x in range(5)] for y in range(n_hidden_num)]\n",
    "\n",
    "for hid in range(n_hidden_num):\n",
    "    \n",
    "    n_hidden = hid+n_hidden_min\n",
    "    print('Number of Nodes in Hidden Layer = ', n_hidden)\n",
    "    \n",
    "    hidden_cv[hid][0] = n_hidden\n",
    "    \n",
    "    \"\"\" k-Fold Cross Validation \"\"\"\n",
    "    w1, w2, Acc = kfold_nn(X_LDA_2D, Y_trn, X_LDA_val, Y_val, n_output, \n",
    "                            n_features, n_hidden, l2, epochs, learning_rate, momentum_const, \n",
    "                          decay_rate, minibatch_size, optimizer, activation, check_gradients, nesterov, print_progress, \n",
    "                          k = 1)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "t1 = time.time()\n",
    "print('Time Taken >> ', t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wt1, wt2 = w1, w2\n",
    "wt1, wt2 = w1, w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Acc_df = pd.DataFrame(Acc)\n",
    "Acc_df.to_csv(\"Solution Trajectory.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65.69343065693431"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(X_LDA_tst, Y_tst, w1, w2, activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.2156779 ,  0.915761  ,  0.36210628,  0.62009538],\n",
       "       [ 0.52767063,  0.86684883,  0.14267155,  0.67198652],\n",
       "       [ 0.99476113,  0.12171233,  0.42696197,  0.58546165]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.ra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
